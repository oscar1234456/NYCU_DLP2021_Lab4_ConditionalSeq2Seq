##
import matplotlib.ticker as ticker
import numpy as np
from os import system
import torch
from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu
from layer import EncoderRNN, DecoderRNN, hiddenCellLinear, ConditionEmbegging
from dataloader import WordTestSet
from utility import reparameter

################################
#Example inputs of compute_bleu
################################
#The target word
reference = 'accessed'
#The word generated by your model
output = 'access'

##compute BLEU-4 score
def compute_bleu(output, reference):
    cc = SmoothingFunction()
    if len(reference) == 3:
        weights = (0.33,0.33,0.33)
    else:
        weights = (0.25,0.25,0.25,0.25)
    return sentence_bleu([reference], output,weights=weights,smoothing_function=cc.method1)

##
"""============================================================================
example input of Gaussian_score

words = [['consult', 'consults', 'consulting', 'consulted'],
['plead', 'pleads', 'pleading', 'pleaded'],
['explain', 'explains', 'explaining', 'explained'],
['amuse', 'amuses', 'amusing', 'amused'], ....]

the order should be : simple present, third person, present progressive, past
============================================================================"""

def Gaussian_score(words):
    words_list = []
    score = 0
    yourpath = 'data/train.txt'#should be your directory of train.txt
    with open(yourpath,'r') as fp:
        for line in fp:
            word = line.split(' ')
            word[3] = word[3].strip('\n')
            words_list.extend([word])
        for t in words:
            for i in words_list:
                if t == i:
                    score += 1
    return score/len(words)

##
def evaluateBLEU(encoder:EncoderRNN, decoder:DecoderRNN, hiddenLinear:hiddenCellLinear,
                 cellLinear:hiddenCellLinear, conditionEmbedding:ConditionEmbegging, condEmbedding_size):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device:{device} Evaluating!")
    print("-" * 5, end='')
    print("Evaluation Begin", end='')
    print("-" * 5)

    SOS_token = 0
    EOS_token = 1
    # all model has already definded its layer size
    # open eval mode
    encoder.eval()
    decoder.eval()
    hiddenLinear.eval()
    cellLinear.eval()
    conditionEmbedding.eval()

    # load testing set
    testingPairs = WordTestSet().getWordPair() #[[[first word(Tensor), tense(Tensor)],[need to convert(String), tense(Tensor)]],...]

    testingNum = len(testingPairs)
    score = 0
    for test_iteration, testingPair in enumerate(testingPairs):
        #testingPair[0]->First word, testingPair[0][0]: word(Tensor), testingPair[0][1]:tense(Tensor)
        #testingPair[1]->Need to Convert, testingPair[1][0]: word(string), testingPair[1][1]:tense(Tensor)
        target_length = len(testingPair[1][0])+1
        reference = testingPair[1][0]
        firstCondition_tensor = testingPair[0][1]
        targetCondition_tensor = testingPair[1][1]
        input_tensor = testingPair[0][0]
        encoder_hidden = encoder.initHidden(condEmbedding_size)
        encoder_cell = encoder.initCell(condEmbedding_size)

        with torch.no_grad():
            firstConditionEmbedded = conditionEmbedding(firstCondition_tensor).view(1, 1, -1)
            targetConditionEmbedded = conditionEmbedding(targetCondition_tensor).view(1, 1, -1)

            firstConditionHidden = torch.cat((encoder_hidden, firstConditionEmbedded), 2)  # concate condition to hidden
            firstConditionCell = torch.cat((encoder_cell, firstConditionEmbedded), 2)  # concate condition to cell

            encoder_hidden = (firstConditionHidden, firstConditionCell)

            hidden_mean, hidden_logVar, cell_mean, cell_logVar = encoder(input_tensor, encoder_hidden)

            hidden_latent = reparameter(hidden_mean, hidden_logVar)
            cell_latent = reparameter(cell_mean, cell_logVar)

            targetCondition_hidden_latent = torch.cat((hidden_latent, targetConditionEmbedded), 2)
            targetCondition_cell_latent = torch.cat((cell_latent, targetConditionEmbedded), 2)

            decoder_input = torch.tensor([[SOS_token]], device=device)

            targetCondition_hidden_latent_toDe = hiddenLinear(targetCondition_hidden_latent)
            targetCondition_cell_latent_toDe = cellLinear(targetCondition_cell_latent)

            decoder_hidden = (targetCondition_hidden_latent_toDe, targetCondition_cell_latent_toDe)

            decorderResult = list()

            for di in range(target_length):
                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)
                # loss += criterion(decoder_output, target_tensor[di])
                topv, topi = decoder_output.topk(1)
                decoder_input = topi.squeeze().detach()
                decorderResult.append(decoder_input)
                if decoder_input.item() == EOS_token:
                    break
        targetWord = WordTestSet.vec2word(decorderResult)
        score += compute_bleu(targetWord, reference)
    print(f"Average BLEU-4 score: {score/testingNum}")
    print("-"*5, end='')
    print("Evaluation Finish", end='')
    print("-" * 5)
    # close eval mode
    encoder.train()
    decoder.train()
    hiddenLinear.train()
    cellLinear.train()
    conditionEmbedding.train()
    return score/testingNum

if __name__ == "__main__":
    testingPairs = WordTestSet().getWordPair()
    print(testingPairs)
